```{r}
load("TX_2.rdata")
options(scipen = 999)
library(fpp3)
library(data.table)
library(stringr)
```

```{r}
## Sales data
product_list <- unique(sales$item_id) 
length(product_list)
unique(sales$dept_id)
# 1437 products, 3 departments

unique(sales$store_id) # only one store

setDT(sales)[, .(count = uniqueN(item_id)), by = dept_id] # number of items per department

View(is.na(sales))
#no missing values in this data set
```

```{r}
total_sales <- sales %>%
  group_by(day) %>%
  summarise(tot_sales = sum(sales)) %>%
  as_tsibble() %>% filter_index("2014-01-01"~ "2016-04-25")

total_sales %>% autoplot()

#there are outliers in the data (26/12)
```

```{r}
#AFC plot: not white noise here, coeff are pretty high/significant, way above confidence interval most of the time: seems to have a strong pattern/seasonality

#total_sales %>% ACF(tot_sales) %>% autoplot()
#seasonal subseries plot: chart by month to see evolution of time series during the years for a particular month
#total_sales %>%
#  mutate(Month = yearmonth(day)) %>%
# filter(Month > yearmonth("2011 Jan") & Month < yearmonth("2016 Apr") ) %>%
#  as_tibble()
```

```{r}
#sales data: from daily to monthly
data_new2 <- as.data.frame(sales)
data_new2$year_month <- floor_date(data_new2$day,"month")

data_aggr2 <- data_new2 %>%
  group_by(year_month) %>%
  summarize(tot_sales = sum(sales)) %>% 
  as_tsibble()

monthly_sales_data <-
  data_aggr2 %>%
  mutate(year_month = yearmonth(year_month)) %>%
  as_tsibble(index = year_month)

monthly_sales_data %>%
  filter(year_month > yearmonth("2011 Jan")) %>% 
  model(STL(tot_sales ~ trend(window = 21))) %>%
  components() %>%
  autoplot()

monthly_sales_data %>%
  filter(year_month > yearmonth("2013 Dec") & year_month < yearmonth("2016 Apr")) %>%
  gg_subseries(tot_sales) + ylab("Total Sales") + xlab("Year")
#we can see that the trend stop decreasing in 2014: we can filter from this date in order to train from this date
#if we go more into the past, the trend will not be appropriate
#training set from 2014
```

```{r}
total_sales %>% model(STL(tot_sales)) %>% components() %>% autoplot()
#STL decomposition of the total sales per day
#USEFUL FOR OUTLIERS

#Visualizing Total Sales over time to see main pattern of our data
total_sales %>% autoplot(tot_sales)
#we can see some outliers
total_sales %>% filter(tot_sales < 1000)
#day        tot_sales
#2011-12-26	0			
#2012-12-26	0
#2013-12-26	11
#2014-03-27	782			can be explain by St Patrick Day (cal row = 1144)
#2014-12-26	7       outliers 1
#2015-03-25	93      outliers 2
#2015-12-26	0       outliers 3

total_sales %>%
  mutate(Month = yearmonth(day)) %>%
  filter(Month >= yearmonth("2014 Jan")) %>% autoplot()

total_sales %>% filter(tot_sales > 5000) #outliers because of NBAFinalsEnd (row 1600 in the cal data)
#day        tot_sales
#2015-06-16	5251	
```

```{r}
sales_ts <- sales %>% as_tsibble(index = day, key = c(store_id, dept_id, item_id)) %>% filter_index("2014-01-01"~.)
#without product in the hierarchical

##hierarchical structure : hierarchy : store -> items
# there are 1437 products. I propose to disregard the lowest hierarchy (the products) because too much products ? Maybe concentrate ourselves on the top 5 selling products?
```

```{r}
#REMOVING OUTLIERS

#keep outliers in memory
outliers <- sales_ts %>%
  filter_index("2014-12-26", "2015-03-25" ,"2015-12-26", "2014-03-27")

sales_ts_miss <- sales_ts %>% 
  anti_join(outliers) %>% #remove outliers
  fill_gaps() #replace them by missing values

sales_ts_fill <- sales_ts_miss %>% # Fit TSLM model to the data containing missing values
  model(TSLM(sales ~ trend())) %>% # Estimate tot_sales for all periods
  interpolate(sales_ts_miss)
sales_ts_fill %>% filter(sales < 0)
sales_ts_fill$sales <- abs(sales_ts_fill$sales)
sales_ts_fill %>% filter(is.na(sales))

#removing outliers in total_sales object
total_ts_sales <- sales_ts_fill %>%
  summarise(tot_sales = sum(sales)) %>%
  as_tsibble() %>% filter_index("2014-01-01"~ "2016-04-25")
```

```{r}
#departement sales vizualizations
total_dep_sales <- sales_ts_fill %>%
  group_by(dept_id) %>%
  summarise(sales = sum(sales))

dep1tot_sales <- total_dep_sales %>%
  filter(dept_id == "FOODS_1") %>%
  as_tsibble()

dep2tot_sales <- total_dep_sales %>%
  filter(dept_id == "FOODS_2") %>%
  as_tsibble()

dep3tot_sales <- total_dep_sales %>%
  filter(dept_id == "FOODS_3") %>%
  as_tsibble()

dep1tot_sales %>% filter_index("2014-01-01"~.) %>% autoplot(sales)
dep2tot_sales %>% filter_index("2014-01-01"~.) %>% autoplot(sales)
dep3tot_sales %>% filter_index("2014-01-01"~.) %>% autoplot(sales)
```

```{r}
dep1tot_sales %>%
  filter_index("2014-01-01"~.) %>%
  filter(sales < 100)

dep2tot_sales %>%
  filter_index("2014-01-01"~.) %>%
  filter(sales < 200)

dep3tot_sales %>%
  filter_index("2014-01-01"~.) %>%
  filter(sales < 1000)

# we can see a sale decrease @ 2014-03-27 in the 3 department
# no explanation in the cal data set
#take this data as outlier
```

```{r}
## Calendar data
unique(cal$type_1) 
# 4 types of events

cal %>%
  filter(name_1 != "") 

# 162 days with events

cal %>%
  mutate(event = name_1 != "") %>%
  mutate(month = strftime(cal$date, format = "%m")) %>%
  filter(event == TRUE)  %>%
  ggplot(aes(month)) +
  geom_histogram(stat="count")

# particularly many events in February and few in August
  

cal %>%
  filter(name_2 != "") 

# 5 days with 2 events, these might appear as outliers in the dataset
cal %>%
  mutate(day = strftime(cal$date, format = "%d")) %>%
  select(day, snap_TX)  %>%
  ggplot(aes(day, snap_TX)) +
  geom_point() 

#snap benefits are distributed in sequence : 1,3,5,6,7,9,11,12,13,15 every month throughout the data set
cal <- cal %>%
  select(-snap_CA, -snap_WI, -wday)
```

```{r}
#cal dataset manipulation: from tibble to tsibble
cal <- cal %>% 
  mutate(date = date(
    as.Date(date,
      format = "%Y-%m-%d"))) %>%
  as_tsibble() #chr to date


cal2014 <- cal %>%
  filter_index("2014-01-01" ~"2016-05-23") %>%
  select(date, name_1, name_2, snap_TX, wm_yr_wk)
```

```{r}
prices <- prices %>%
  filter_all(any_vars(str_detect(., pattern = "FOOD")))

prices %>%
  ggplot(aes(wm_yr_wk, sell_price)) +
  geom_point()

#we can see the evolution of the price of any product, we  need to transform the wm_yr_wk variable to get a continuous time variable
```

```{r}
cal_price <- full_join(as_tibble(cal), as_tibble(prices), by = "wm_yr_wk") %>% select(-wm_yr_wk)
```

```{r}
#creation of snap benefits dummy variable
total_ts_sales$snap <-
  ifelse(
    day(total_sales$day) == 1 |
      day(total_sales$day) == 3 |
      day(total_sales$day) == 5 |
      day(total_sales$day) == 6 |
      day(total_sales$day) == 7 |
      day(total_sales$day) == 9 |
      day(total_sales$day) == 11 |
      day(total_sales$day) == 12 |
      day(total_sales$day) == 13 |
      day(total_sales$day) == 15,
    "TRUE",
    "FALSE"
  )
#or O and 1
```

```{r}
#merging the sales & cal data set
#creation of the data set used for what model to use
total_ts_sales <- total_ts_sales %>% rename(date = day)
df <- full_join(as_tibble(cal2014), as_tibble(total_ts_sales), by = "date")

df <- df %>% as_tsibble(index = date)
```

```{r}
#creation of the data set used for the aggregation part
sales_ts_fill <- sales_ts_fill %>% rename(date = day)
df2 <- full_join(as_tibble(cal2014), as_tibble(sales_ts_fill), by = "date")
df2$name_1 <- ifelse(df2$name_1 == "", "NOEVENT",df2$name_1)
df3 <- df2 %>% as_tsibble(index = date, key = c(store_id, dept_id, item_id, name_1, snap_TX))
df2 <- df2 %>% as_tsibble(index = date, key = c(store_id, dept_id, item_id))


```

```{r}
#creation of events dummy variable
df <-
  fastDummies::dummy_cols(df, select_columns = c("name_1", "name_2"), remove_first_dummy = TRUE) %>%
  select(-name_1, -name_2, -snap_TX)
df <- df %>% as_tsibble()
```

```{r}
#MEAN + NAIVE + SNAIVE + ETS
sales_fit <- total_ts_sales %>%
  model(
    Mean = MEAN(tot_sales),
    `Naïve` = NAIVE(tot_sales),
    `Seasonal naïve` = SNAIVE(tot_sales),
    ETS = ETS(tot_sales)
  )

sales_fit %>%
  forecast(total_ts_sales %>% filter_index(.~"2015-04-25"), h = 28) %>%
  autoplot(total_ts_sales %>% filter_index("2016"~.))
```

```{r}
#we'll often use this code in order to generate new data
#instead of copy-paste this ugly code,
#just assign to a new object
dummy_snap = ifelse(
      day(df[847:874,]$date) == 1 |
        day(df[847:874,]$date) == 3 |
        day(df[847:874,]$date) == 5 |
        day(df[847:874,]$date) == 6 |
        day(df[847:874,]$date) == 7 |
        day(df[847:874,]$date) == 9 |
        day(df[847:874,]$date) == 11 |
        day(df[847:874,]$date) == 12 |
        day(df[847:874,]$date) == 13 |
        day(df[847:874,]$date) == 15,
      "TRUE",
      "FALSE"
    )
```

```{r}
# LINEAR REGRESSION WITH: snap dummy
fit_lm <- df %>% filter_index(.~"2015-04-25") %>% 
          model(TSLM(tot_sales ~ trend() + season() + snap))

newdata_TLSM_snap <- new_data(total_ts_sales %>% filter_index(.~"2015-04-25"), n = 28) %>%
  mutate(snap = dummy_snap)
fit_lm %>%
  forecast(new_data = newdata_TLSM_snap) %>%
  autoplot(total_ts_sales %>% filter_index("2015-04"~"2015-07"))
```

```{r}
# LINEAR REGRESSION WITH: event dummy that occurs during h=28  + snap
fit_all_lm <- df %>% filter_index(.~"2015-04-25") %>% 
  model(TSLM(
      tot_sales ~ trend() +
        season() + snap +
        `name_1_Mother's day` +
        name_1_OrthodoxEaster + 
        `name_1_Pesach End` + `name_1_Cinco De Mayo`
  ))

newdata_TLSM_snap_h28event <-
  new_data(total_ts_sales %>% filter_index(.~"2015-04-25"), n = 28) %>%
  mutate(
    `name_1_Cinco De Mayo` = df[847:874,]$`name_1_Cinco De Mayo`,
    `name_1_Mother's day` =
      ifelse(df[847:874,]$`name_1_Mother's day` == 1, "TRUE", "FALSE"),
    name_1_OrthodoxEaster =
      ifelse(df[847:874,]$name_1_OrthodoxEaster == 1, "TRUE", "FALSE"),
    `name_1_Pesach End` =
      ifelse(df[847:874,]$`name_1_Pesach End` == 1, "TRUE", "FALSE"),
    snap = dummy_snap
  )

fit_all_lm %>%
  forecast(new_data = newdata_TLSM_snap_h28event) %>%
  autoplot(total_ts_sales %>% filter_index("2015-04"~"2015-07"))
```

```{r}
# mod will be useful in order to have a clean code
#avoid copy paste model tuning
df[5:36] <- sapply(df[5:36], as.logical)
paste_col <- paste0(colnames(df)[5:36])
mod <- paste("tot_sales ~ ", paste(paste_col, collapse = " + "))
```

```{r}
#LINEAR REGRESSION take into account all the event
fit_generalized <- df %>% filter_index(.~"2015-04-25") %>%
  model(
    TSLM(
      tot_sales ~ trend() +
        season() + snap +
        `name_1_Chanukah End` +
        name_1_Christmas +
        `name_1_Cinco De Mayo`+
        name_1_ColumbusDay +
        name_1_Easter +
        `name_1_Eid al-Fitr` +
        name_1_EidAlAdha +
        `name_1_Father's day` +
        name_1_Halloween +
        name_1_IndependenceDay +
        name_1_LaborDay +
        name_1_LentStart +
        name_1_LentWeek2 +
        name_1_MartinLutherKingDay +
        name_1_MemorialDay +
        `name_1_Mother's day` +
        name_1_NBAFinalsEnd +
        name_1_NBAFinalsStart +
        name_1_NewYear +
        name_1_OrthodoxChristmas +
        name_1_OrthodoxEaster +
        `name_1_Pesach End` +
        name_1_PresidentsDay +
        `name_1_Purim End` +
        `name_1_Ramadan starts` +
        name_1_StPatricksDay +
        name_1_SuperBowl +
        name_1_Thanksgiving +
        name_1_ValentinesDay +
        name_1_VeteransDay +
        `name_2_Father's day` +
        name_2_OrthodoxEaster
    )
  )

report(fit_generalized)

fit_generalized_newdata <-
  cbind(new_data(total_ts_sales %>% filter_index(.~"2015-04-25"), n = 28), df[847:874, 5:36]) %>% mutate(
    snap = dummy_snap) %>% as_tsibble()

fit_generalized %>%
  forecast(new_data = fit_generalized_newdata) %>%
  autoplot(total_ts_sales %>% filter_index("2015-04"~"2015-07"))
```

```{r}
#LINEAR REGRESSION but removing all non-significant variable
fit_generalized_backward <- df %>% filter_index(.~"2015-04-25") %>%
  model(
    TSLM(
      tot_sales ~ trend() +
        season() +
        name_1_Christmas +
        name_1_Easter +
        name_1_IndependenceDay +
        name_1_LaborDay +
        name_1_MemorialDay +
        name_1_NBAFinalsEnd +
        name_1_NewYear +
        `name_1_Ramadan starts` +
        name_1_SuperBowl +
        name_1_Thanksgiving +
        name_1_ValentinesDay +
        `name_2_Father's day`
    )
  )

report(fit_generalized_backward)

fit_generalized_backward_newdata <-
  cbind(new_data(total_ts_sales %>% filter_index(.~"2015-04-25"), n = 28), df[847:874, 5:36]) %>% mutate(
    snap = dummy_snap) %>% as_tsibble()

fit_generalized_backward %>%
  forecast(new_data = fit_generalized_backward_newdata) %>%
  autoplot(total_ts_sales %>% filter_index("2015-04"~"2015-07"))
```

```{r}
#ARIMA model -> just with snap dummy
fit_ARIMA1 <- df %>% filter_index(.~"2015-04-25") %>% 
          model(ARIMA(tot_sales ~ trend() + season() + snap))

fit_ARIMA1 %>%
  forecast(new_data = fit_generalized_backward_newdata) %>%
  autoplot(total_ts_sales %>% filter_index("2015-04"~"2015-07"))
```

```{r}
#ARIMA model with all event in order to generalize
fit_ARIMA1_1 <- df %>% filter_index(.~"2015-04-25") %>% 
          model(ARIMA(tot_sales ~ trend() + season() + snap +
        `name_1_Chanukah End` +
        name_1_Christmas +
        `name_1_Cinco De Mayo`+
        name_1_ColumbusDay +
        name_1_Easter +
        `name_1_Eid al-Fitr` +
        name_1_EidAlAdha +
        `name_1_Father's day` +
        name_1_Halloween +
        name_1_IndependenceDay +
        name_1_LaborDay +
        name_1_LentStart +
        name_1_LentWeek2 +
        name_1_MartinLutherKingDay +
        name_1_MemorialDay +
        `name_1_Mother's day` +
        name_1_NBAFinalsEnd +
        name_1_NBAFinalsStart +
        name_1_NewYear +
        name_1_OrthodoxChristmas +
        name_1_OrthodoxEaster +
        `name_1_Pesach End` +
        name_1_PresidentsDay +
        `name_1_Purim End` +
        `name_1_Ramadan starts` +
        name_1_StPatricksDay +
        name_1_SuperBowl +
        name_1_Thanksgiving +
        name_1_ValentinesDay +
        name_1_VeteransDay +
        `name_2_Father's day` +
        name_2_OrthodoxEaster
    )
  )


fit_ARIMA1_1 %>%
  forecast(new_data = fit_generalized_newdata) %>%
  autoplot(total_ts_sales %>% filter_index("2015-04"~"2015-07"))
```

```{r}
backward_arima <- df %>% filter_index(.~"2015-04-25") %>%
  model(
    ARIMA(
      tot_sales ~ trend() +
        season() +
        name_1_Christmas +
        name_1_Easter +
        name_1_IndependenceDay +
        name_1_LaborDay +
        name_1_MemorialDay +
        name_1_NBAFinalsEnd +
        name_1_NewYear +
        `name_1_Ramadan starts` +
        name_1_SuperBowl +
        name_1_Thanksgiving +
        name_1_ValentinesDay +
        `name_2_Father's day`
    )
  )

backward_arima %>%
  forecast(new_data = fit_generalized_newdata) %>%
  autoplot(total_ts_sales %>% filter_index("2015-04"~"2015-07"))

```

```{r}
#compare the forecasts from different models, which one provides forecasts closest to real data
tr <- total_ts_sales %>% filter(date <= "2016-03-27")
tr2 <- tr %>% model(ets = ETS(tot_sales), 
                    tslm = TSLM(tot_sales ~ trend() + season()), 
                    arima = ARIMA(tot_sales))
tr_fc <- tr2 %>% forecast(h = 28) 
tr_fc %>% autoplot(total_ts_sales %>% filter_index("2016"~.))
tr_fc %>% accuracy(total_ts_sales) #check accuracy of the models
```

```{r}
bind_rows(accuracy(sales_fit),
accuracy(fit_lm),
accuracy(fit_all_lm),
accuracy(fit_generalized),
accuracy(fit_generalized_backward),
accuracy(fit_ARIMA1),
accuracy(fit_ARIMA1_1),
accuracy(backward_arima)) %>% arrange(MASE)
```

```{r}
#As we have struggled to forecast with bottom-up (low computational power of laptop)
#Change the hierarchical way of thinking:
#forecast the total sales in order to estimate the product individually
#top-down approach
# Parent = store
# Child = item_id

#we've tried some CV but out of memory: Error: vector memory exhausted (limit reached?)
#sales_aggr.tr <- df2 %>% 
#  stretch_tsibble(.init = 2, .step = 1) %>%
#  aggregate_key((store_id/item_id), sales = sum(sales))


sales_aggr <- df2 %>% #since the data set is huge, we decide not to CV
  aggregate_key((store_id/item_id), sales = sum(sales)) 

sales_aggr %>%
  filter(!is_aggregated(store_id), !is_aggregated(item_id)) %>%
  autoplot(sales) +
  ylab("Sales") +
  theme(legend.position =
          "none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

sales_aggr %>%
  filter(is_aggregated(item_id)) %>%
  autoplot(sales) +
  facet_wrap(vars(store_id), scales = "free_y", ncol = 2) +
  theme(legend.position = "none")
```

```{r message=TRUE, warning=TRUE}
#MODEL TRAINING

#Top down reconciliation requires strictly hierarchical structures
#we can't add dummys if we've to forecast individual items
#we've to ask max if we can add dummys into a top down approach
#but i dont think so
#-> in order to forecast ind. product : ARIMA with top-down approach

#WARNING : sooooo much timeeee but it works ;) (15-20 min)
fit_aggr <- sales_aggr %>%
  filter_index("2015-03-26"~"2015-04-25") %>% 
  model(base = ARIMA(sales)) %>% reconcile(td = top_down(base))


fc <- fit_aggr %>% forecast(h = 28)

fc %>%
  filter(is_aggregated(item_id)) %>%
  autoplot(
    sales_aggr %>% filter_index("2015-04"~"2015-07")
  ) #visual check of our prediction on total sales

fc %>%
  filter(!is_aggregated(item_id)) %>%
  filter(item_id == "FOODS_1_001") %>% 
  autoplot(
    sales_aggr %>% filter_index("2016"~.) 
  ) # visual check of our prediction on some individual items

a <- fc %>%
  filter(!is_aggregated(item_id)) %>% 
  accuracy(data = sales_aggr) %>% select(-MPE, -MAPE, -ACF1 )
lapply(a[5:9], median) #median of forecasting score for individual item
#0.889239 MASE for individual prediction -> not bad
#RMSSE  = 0.7113033 -> quite good!

fc %>%
  filter(is_aggregated(item_id)) %>% 
  accuracy(data = sales_aggr)
```

```{r message=TRUE, warning=TRUE}
#other model try
#naive
fit_aggr_naive <- sales_aggr %>%
  filter_index("2015-03-26"~"2015-04-25") %>% 
  model(base = SNAIVE(sales)) %>% reconcile(td = top_down(base))
fc_naive <- fit_aggr_naive %>% forecast(h = 28)
fc_naive %>%
  filter(is_aggregated(item_id)) %>%
  autoplot(
    sales_aggr %>% filter_index("2015-04"~"2015-07")
  )
fc_naive %>%
  filter(is_aggregated(item_id)) %>% 
  accuracy(data = sales_aggr) %>% select(MASE)
b <- fc_naive %>%
  filter(!is_aggregated(item_id)) %>% 
  accuracy(data = sales_aggr) %>% select(-MPE, -MAPE, -ACF1 )
lapply(b[5:9], median)
#$RMSE [1] 1.336306
#$MAE[1] 0.9285714
#$MASE[1] 1.031888
#$RMSSE[1] 0.9169868



#ets
fit_aggr_ets <- sales_aggr %>%
  filter_index("2015-03-26"~"2015-04-25") %>% 
  model(base = ETS(sales)) %>% reconcile(td = top_down(base))
fc_ets <- fit_aggr_ets %>% forecast(h = 28)
fc_ets %>%
  filter(is_aggregated(item_id)) %>%
  autoplot(
    sales_aggr %>% filter_index("2015-04"~"2015-07")
  )
fc_ets %>%
  filter(is_aggregated(item_id)) %>% 
  accuracy(data = sales_aggr) %>% select(MASE) #MASE=1.04
c <- fc_ets %>%
  filter(!is_aggregated(item_id)) %>% 
  accuracy(data = sales_aggr) %>% select(-MPE, -MAPE, -ACF1 )
lapply(c[5:9], median)
#$RMSE [1] 1.011418
#$MAE[1] 0.7881585
#$MASE[1] 0.9013102
#$RMSSE[1] 0.70817

```


```{r message=TRUE, warning=TRUE}
#MODEL FITTING
fit_aggr.fit <- sales_aggr %>%
  filter_index("2016-03-26"~"2016-04-25") %>% 
  model(base = ARIMA(sales)) %>% reconcile(td = top_down(base))


fc.fit <- fit_aggr.fit %>% forecast(h = 28)

fc.fit %>%
  filter(is_aggregated(item_id)) %>%
  autoplot(
    sales_aggr %>% filter_index("2015-04"~"2015-07")
  ) 

#not the best/ more efficient model, but it works fine
#why not using GG cloud for more computational power?
```

```{r message=TRUE, warning=TRUE}
# what we've tried:

#bottom-up item_id -> dept_id
#bottom-up item_id -> store_id
#poor prediction

#implementing dummys in our top-down
#didnt find a way

#the model that we're presenting is the best model that we've find while taking into account our computational capacity
```


